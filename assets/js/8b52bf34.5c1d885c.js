"use strict";(self.webpackChunkrage_4_j=self.webpackChunkrage_4_j||[]).push([[233],{9322:(e,r,n)=>{n.r(r),n.d(r,{assets:()=>c,contentTitle:()=>t,default:()=>u,frontMatter:()=>l,metadata:()=>s,toc:()=>a});const s=JSON.parse('{"id":"rage4j-core/metrics/rouge_score","title":"ROUGE Score","description":"Evaluator: RougeScoreEvaluator","source":"@site/docs/rage4j-core/metrics/rouge_score.md","sourceDirName":"rage4j-core/metrics","slug":"/rage4j-core/metrics/rouge_score","permalink":"/rage4j/docs/rage4j-core/metrics/rouge_score","draft":false,"unlisted":false,"editUrl":"https://github.com/explore-de/rage4j/docs/rage4j-core/metrics/rouge_score.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"BLEU Score","permalink":"/rage4j/docs/rage4j-core/metrics/bleu_score"},"next":{"title":"RAGE4j-Assert","permalink":"/rage4j/docs/category/rage4j-assert"}}');var o=n(4848),i=n(8453);const l={},t="ROUGE Score",c={},a=[{value:"Supported ROUGE Types",id:"supported-rouge-types",level:2},{value:"Supported Measure Types",id:"supported-measure-types",level:2},{value:"Required Sample Fields",id:"required-sample-fields",level:2},{value:"How It Works",id:"how-it-works",level:2},{value:"Score Interpretation",id:"score-interpretation",level:2},{value:"Example Usage",id:"example-usage",level:2}];function d(e){const r={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(r.header,{children:(0,o.jsx)(r.h1,{id:"rouge-score",children:"ROUGE Score"})}),"\n",(0,o.jsxs)(r.p,{children:[(0,o.jsx)(r.strong,{children:"Evaluator"}),": ",(0,o.jsx)(r.code,{children:"RougeScoreEvaluator"})]}),"\n",(0,o.jsx)(r.p,{children:"The ROUGE score evaluates the content overlap between an LLM-generated answer and a ground truth reference. It is\ncommonly used for tasks like summarization and text generation. This evaluator supports multiple ROUGE variants,\nas well as different scoring methods (Precision, Recall, F1 score)."}),"\n",(0,o.jsxs)(r.p,{children:["By default, it uses ",(0,o.jsx)(r.strong,{children:"ROUGE-1 F1 score"}),", which balances unigram precision and recall."]}),"\n",(0,o.jsx)(r.h2,{id:"supported-rouge-types",children:"Supported ROUGE Types"}),"\n",(0,o.jsxs)(r.ul,{children:["\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.code,{children:"ROUGE-1"})," \u2013 Unigram overlap (individual words)"]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.code,{children:"ROUGE-2"})," \u2013 Bigram overlap (pairs of words)"]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.code,{children:"ROUGE-L"})," \u2013 Longest common subsequence (LCS)"]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.code,{children:"ROUGE_L_SUM"})," \u2013 LCS-based score optimized for multi-sentence summaries (used in summarization tasks)"]}),"\n"]}),"\n",(0,o.jsx)(r.h2,{id:"supported-measure-types",children:"Supported Measure Types"}),"\n",(0,o.jsxs)(r.ul,{children:["\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.code,{children:"PRECISION"})," \u2013 Proportion of matched units in the answer"]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.code,{children:"RECALL"})," \u2013 Proportion of matched units in the reference"]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.code,{children:"F1SCORE"})," \u2013 Harmonic mean of precision and recall"]}),"\n"]}),"\n",(0,o.jsx)(r.h2,{id:"required-sample-fields",children:"Required Sample Fields"}),"\n",(0,o.jsxs)(r.ul,{children:["\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.code,{children:"answer"})," \u2013 Required for n-gram or sequence overlap comparison."]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.code,{children:"groundTruth"})," \u2013 Required for n-gram or sequence overlap comparison."]}),"\n"]}),"\n",(0,o.jsx)(r.h2,{id:"how-it-works",children:"How It Works"}),"\n",(0,o.jsxs)(r.ol,{children:["\n",(0,o.jsx)(r.li,{children:"Splits both the answer and the ground truth into tokens based on whitespace."}),"\n",(0,o.jsxs)(r.li,{children:["Based on the selected ",(0,o.jsx)(r.code,{children:"RougeType"}),":","\n",(0,o.jsxs)(r.ul,{children:["\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.code,{children:"ROUGE-1"})," and ",(0,o.jsx)(r.code,{children:"ROUGE-2"})," compare n-gram overlap."]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.code,{children:"ROUGE-L"})," computes the longest common subsequence (LCS)."]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.code,{children:"ROUGE_L_SUM"})," groups the tokens into sentences using '\\n' as sentence boundary and applies LCS across all answer\nand ground truth sentence pairs."]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(r.li,{children:["Calculates:","\n",(0,o.jsxs)(r.ul,{children:["\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.strong,{children:"Precision"}),": proportion of overlapping units in the answer."]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.strong,{children:"Recall"}),": proportion of overlapping units in the reference."]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.strong,{children:"F1 Score"}),": harmonic mean of precision and recall."]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(r.li,{children:"Returns the selected metric as the final score."}),"\n"]}),"\n",(0,o.jsx)(r.h2,{id:"score-interpretation",children:"Score Interpretation"}),"\n",(0,o.jsxs)(r.ul,{children:["\n",(0,o.jsxs)(r.li,{children:["\n",(0,o.jsxs)(r.p,{children:[(0,o.jsx)(r.strong,{children:"Range"}),": ",(0,o.jsx)(r.code,{children:"0.0"})," (no overlap) to ",(0,o.jsx)(r.code,{children:"1.0"})," (perfect match)"]}),"\n"]}),"\n",(0,o.jsxs)(r.li,{children:["\n",(0,o.jsxs)(r.p,{children:[(0,o.jsx)(r.strong,{children:"ROUGE-1"})," \u2013 A high score indicates strong alignment with the important words used in the reference."]}),"\n"]}),"\n",(0,o.jsxs)(r.li,{children:["\n",(0,o.jsxs)(r.p,{children:[(0,o.jsx)(r.strong,{children:"ROUGE-2"})," \u2013 A high score suggests the answer closely follows the phrasing and local word order of the reference."]}),"\n"]}),"\n",(0,o.jsxs)(r.li,{children:["\n",(0,o.jsxs)(r.p,{children:[(0,o.jsx)(r.strong,{children:"ROUGE-L"})," \u2013 A high score reflects that the answer preserves the overall structure and ordering of content from the\nreference."]}),"\n"]}),"\n",(0,o.jsxs)(r.li,{children:["\n",(0,o.jsxs)(r.p,{children:[(0,o.jsx)(r.strong,{children:"ROUGE-LSum"})," \u2013 A high score means the answer captures the main ideas and preserves the overall flow of information\nacross sentences."]}),"\n"]}),"\n",(0,o.jsxs)(r.li,{children:["\n",(0,o.jsx)(r.p,{children:"ROUGE is well suited for evaluating longer or free-form text, but like BLEU, it may miss semantic similarity when phrasing differs."}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(r.h2,{id:"example-usage",children:"Example Usage"}),"\n",(0,o.jsx)(r.pre,{children:(0,o.jsx)(r.code,{className:"language-java",children:"// Default: ROUGE-1 F1 score\nRougeScoreEvaluator evaluator = new RougeScoreEvaluator();\n\n// Custom: ROUGE-2 Precision\nRougeScoreEvaluator customEvaluator = new RougeScoreEvaluator(RougeType.ROUGE2, MeasureType.PRECISION);\n\n// Custom: ROUGE-LSum Recall\nRougeScoreEvaluator summaryEvaluator = new RougeScoreEvaluator(RougeType.ROUGE_L_SUM, MeasureType.RECALL);\n\nEvaluation result = evaluator.evaluate(sample);\ndouble rougeScore = result.getValue();\n"})})]})}function u(e={}){const{wrapper:r}={...(0,i.R)(),...e.components};return r?(0,o.jsx)(r,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,r,n)=>{n.d(r,{R:()=>l,x:()=>t});var s=n(6540);const o={},i=s.createContext(o);function l(e){const r=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(r):{...r,...e}}),[r,e])}function t(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:l(e.components),s.createElement(i.Provider,{value:r},e.children)}}}]);