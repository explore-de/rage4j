"use strict";(self.webpackChunkrage_4_j=self.webpackChunkrage_4_j||[]).push([[424],{6179:(e,r,n)=>{n.r(r),n.d(r,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"rage4j-core/metrics/bleu_score","title":"BLEU Score","description":"Evaluator: BleuScoreEvaluator","source":"@site/docs/rage4j-core/metrics/bleu_score.md","sourceDirName":"rage4j-core/metrics","slug":"/rage4j-core/metrics/bleu_score","permalink":"/rage4j/docs/rage4j-core/metrics/bleu_score","draft":false,"unlisted":false,"editUrl":"https://github.com/explore-de/rage4j/docs/rage4j-core/metrics/bleu_score.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Answer Semantic Similarity","permalink":"/rage4j/docs/rage4j-core/metrics/answer_semantic_similarity"},"next":{"title":"ROUGE Score","permalink":"/rage4j/docs/rage4j-core/metrics/rouge_score"}}');var s=n(4848),i=n(8453);const o={},a="BLEU Score",l={},c=[{value:"Required Sample Fields",id:"required-sample-fields",level:2},{value:"How It Works",id:"how-it-works",level:2},{value:"Score Interpretation",id:"score-interpretation",level:2},{value:"Example Usage",id:"example-usage",level:2}];function d(e){const r={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(r.header,{children:(0,s.jsx)(r.h1,{id:"bleu-score",children:"BLEU Score"})}),"\n",(0,s.jsxs)(r.p,{children:[(0,s.jsx)(r.strong,{children:"Evaluator"}),": ",(0,s.jsx)(r.code,{children:"BleuScoreEvaluator"})]}),"\n",(0,s.jsxs)(r.p,{children:["The BLEU score evaluates the similarity between an LLM-generated answer and a ground truth reference by computing the\n",(0,s.jsx)(r.strong,{children:"weighted geometric mean of clipped n-gram precisions"})," (from unigrams up to 4-grams). It includes a ",(0,s.jsx)(r.strong,{children:"brevity penalty"}),"\nto penalize overly short answers that might otherwise score high due to precision alone."]}),"\n",(0,s.jsxs)(r.p,{children:[(0,s.jsx)(r.strong,{children:"Modified precision"})," measures how many of the answer\u2019s n-grams appear in the ground truth but limits repeated matches\nto avoid overcounting."]}),"\n",(0,s.jsx)(r.h2,{id:"required-sample-fields",children:"Required Sample Fields"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.code,{children:"answer"})," \u2013 Required for n-gram overlap comparison."]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.code,{children:"groundTruth"})," \u2013 Required for n-gram overlap comparison."]}),"\n"]}),"\n",(0,s.jsx)(r.h2,{id:"how-it-works",children:"How It Works"}),"\n",(0,s.jsxs)(r.ol,{children:["\n",(0,s.jsx)(r.li,{children:"Splits both the answer and the ground truth into tokens based on whitespace."}),"\n",(0,s.jsx)(r.li,{children:"Calculates modified n-gram overlap precision (with clipping) between the answer and the ground truth for n = 1 to 4."}),"\n",(0,s.jsx)(r.li,{children:"Computes the geometric mean of these n-gram precisions, with smoothing."}),"\n",(0,s.jsxs)(r.li,{children:["Applies a ",(0,s.jsx)(r.strong,{children:"brevity penalty"})," to reduce the score if the answer is significantly shorter than the ground truth."]}),"\n"]}),"\n",(0,s.jsx)(r.h2,{id:"score-interpretation",children:"Score Interpretation"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Range"}),": ",(0,s.jsx)(r.code,{children:"0.0"})," (no overlap) to ",(0,s.jsx)(r.code,{children:"1.0"})," (perfect match)"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Higher scores"})," indicate closer word-level similarity between the answer and the reference, with better preservation\nof multi-word phrases."]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Lower scores"})," may indicate missing content, incorrect phrasing, or overly short answers."]}),"\n",(0,s.jsx)(r.li,{children:"BLEU is best suited for comparing structured or templated outputs where exact phrasing matters, but it may not capture\nsemantic similarity well in more flexible, creative text."}),"\n"]}),"\n",(0,s.jsx)(r.h2,{id:"example-usage",children:"Example Usage"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-java",children:"BleuScoreEvaluator evaluator = new BleuScoreEvaluator();\nEvaluation result = evaluator.evaluate(sample);\ndouble bleuScore = result.getValue();\n"})})]})}function h(e={}){const{wrapper:r}={...(0,i.R)(),...e.components};return r?(0,s.jsx)(r,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,r,n)=>{n.d(r,{R:()=>o,x:()=>a});var t=n(6540);const s={},i=t.createContext(s);function o(e){const r=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(r):{...r,...e}}),[r,e])}function a(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),t.createElement(i.Provider,{value:r},e.children)}}}]);